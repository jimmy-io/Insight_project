{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "NUM_CLASSES = 228\n",
    "STARTER_LEARNING_RATE = 0.005\n",
    "CUT_OFF = 0.184\n",
    "DECAY_STEPS = 400000\n",
    "DECAY_RATE = 0.5\n",
    "\n",
    "def alexnet_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for Alexnet.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    input_layer = tf.convert_to_tensor(features[\"x\"])\n",
    "    #print(\"input_layer: {}\".format(input_layer.shape))\n",
    "\n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer,filters=96,kernel_size=[11, 11],strides=4,padding=\"valid\",activation=tf.nn.relu)\n",
    "    #print(\"conv1: {}\".format(conv1.shape))\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[3, 3], strides=2, padding='valid')\n",
    "    #print(\"pool1: {}\".format(pool1.shape))\n",
    "\n",
    "    conv2 = tf.layers.conv2d(inputs= pool1,filters=256,kernel_size=[5, 5],padding=\"same\",activation=tf.nn.relu)\n",
    "    #print(\"conv2: {}\".format(conv2.shape))\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[3, 3], strides=2, padding='valid')\n",
    "    #print(\"pool2: {}\".format(pool2.shape))\n",
    "\n",
    "    conv3 = tf.layers.conv2d(inputs=pool2,filters=384,kernel_size=[3, 3],padding=\"same\",activation=tf.nn.relu)\n",
    "    #print(\"conv3: {}\".format(conv3.shape))\n",
    "\n",
    "    conv4 = tf.layers.conv2d(inputs=conv3,filters=384,kernel_size=[3, 3],padding=\"same\",activation=tf.nn.relu)\n",
    "    #print(\"conv4: {}\".format(conv4.shape))\n",
    "\n",
    "    conv5 = tf.layers.conv2d(inputs=conv4,filters=256,kernel_size=[3, 3],padding=\"same\",activation=tf.nn.relu)\n",
    "    #print(\"conv5: {}\".format(conv5.shape))\n",
    "\n",
    "    pool5 = tf.layers.max_pooling2d(inputs=conv5, pool_size=[3, 3], strides=2,padding='valid')\n",
    "    #print(\"pool5: {}\".format(pool2.shape))\n",
    "\n",
    "    pool5_flat = tf.reshape(conv5, [-1, 12*12*256])\n",
    "    #print(\"pool5_flat: {}\".format(pool5_flat.shape))\n",
    "\n",
    "    fc6 = tf.layers.dense(inputs=pool5_flat, units=4096, activation=tf.nn.relu)\n",
    "    #print(\"dense1: {}\".format(fc6.shape))  \n",
    "\n",
    "    dropout6 = tf.layers.dropout(inputs=fc6, rate=0.2, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    #print(\"dropout6: {}\".format(dropout6.shape))\n",
    "\n",
    "    fc7 = tf.layers.dense(inputs=dropout6, units=4096, activation=tf.nn.relu)\n",
    "    #print(\"fc7: {}\".format(fc7.shape))\n",
    "\n",
    "    dropout7 = tf.layers.dropout(inputs=fc7, rate=0.2, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    #print(\"dropout7: {}\".format(dropout7.shape))\n",
    "\n",
    "    # Logits Layer\n",
    "    # Input Tensor Shape: [batch_size, 4096]\n",
    "    # Output Tensor Shape: [batch_size, 228]\n",
    "    logits = tf.layers.dense(inputs=dropout7, units=NUM_CLASSES)\n",
    "    #print(\"logits: {}\".format(logits.shape))\n",
    "\n",
    "    # Generate Predictions\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.cast(tf.sigmoid(logits) >= CUT_OFF, tf.int8, name=\"class_tensor\"),\n",
    "        # Add `sigmoid_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.sigmoid(logits, name=\"prob_tensor\")\n",
    "    } \n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    #w_tensor = tf.convert_to_tensor(w)\n",
    "    #w_tensor = tf.reshape(w_tensor, [-1,228])\n",
    "    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=logits)#, weights=w_tensor)\n",
    "\n",
    "    #loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            learning_rate=STARTER_LEARNING_RATE, global_step=global_step,\n",
    "            decay_steps=DECAY_STEPS, decay_rate=DECAY_RATE\n",
    "        )\n",
    "        if global_step % DECAY_STEPS == 0:\n",
    "            tf.logging.info('Learning rate at global step '+str(global_step)+': '+str(learning_rate))\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Customize evaluation metric\n",
    "    def meanfscore(predictions, labels):\n",
    "        predictions = tf.reshape(tf.transpose(predictions), [-1])\n",
    "        labels = tf.convert_to_tensor(labels)\n",
    "        labels = tf.reshape(tf.transpose(labels), [-1])\n",
    "        precision_micro, update_op_p = tf.metrics.precision(labels, predictions)\n",
    "        recall_micro, update_op_r = tf.metrics.recall(labels, predictions)\n",
    "        f1_mircro = tf.div(tf.multiply(2., tf.multiply(precision_micro, recall_micro)), tf.add(precision_micro, recall_micro), name=\"eval_tensor\")\n",
    "        return f1_mircro, tf.group(update_op_p, update_op_r)\n",
    "    \n",
    "    def precision_micro(predictions, labels):\n",
    "        predictions = tf.reshape(tf.transpose(predictions), [-1])\n",
    "        labels = tf.convert_to_tensor(labels)\n",
    "        labels = tf.reshape(tf.transpose(labels), [-1])\n",
    "        precision_micro, update_op_p = tf.metrics.precision(labels, predictions)\n",
    "        return precision_micro, update_op_p\n",
    "    \n",
    "    def recall_micro(predictions, labels):\n",
    "        predictions = tf.reshape(tf.transpose(predictions), [-1])\n",
    "        labels = tf.convert_to_tensor(labels)\n",
    "        labels = tf.reshape(tf.transpose(labels), [-1])\n",
    "        recall_micro, update_op_r = tf.metrics.recall(labels, predictions)\n",
    "        return recall_micro, update_op_r\n",
    "    \n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"meanfscore\": meanfscore(predictions[\"classes\"], labels),\n",
    "        \"precision_micro\": precision_micro(predictions[\"classes\"], labels),\n",
    "        \"recall_micro\": recall_micro(predictions[\"classes\"], labels)}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "NUM_CLASSES = 228\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "\n",
    "def load_images(addrs_list):   \n",
    "    images = np.empty((len(addrs_list), IMAGE_WIDTH, IMAGE_HEIGHT, 3), dtype=np.float32)\n",
    "    for i, fpath in enumerate(addrs_list):\n",
    "        img = cv2.imread(fpath, cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        images[i, ...] = img#.transpose(2, 0, 1) \n",
    "        if i % 1000 == 0:\n",
    "            print('Loading images: {}'.format(i))\n",
    "    return images\n",
    "\n",
    "def get_multi_hot_labels(df, index_list):\n",
    "    label_id = [df['labelId'][i] for i in index_list]\n",
    "    \n",
    "    labels_matrix = np.zeros([len(index_list), NUM_CLASSES], dtype=np.uint8())\n",
    "    \n",
    "    for i in range(len(label_id)):\n",
    "        for j in range(len(label_id[i].split(' '))):\n",
    "            row, col = i, int(label_id[i].split(' ')[j]) - 1\n",
    "            labels_matrix[row][col] = 1\n",
    "    \n",
    "    return labels_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images: 0\n",
      "Loading images: 1000\n",
      "Loading images: 2000\n",
      "Loading images: 3000\n",
      "Loading images: 4000\n",
      "Loading images: 5000\n",
      "Loading images: 6000\n",
      "Loading images: 7000\n",
      "Loading images: 8000\n",
      "Loading images: 9000\n"
     ]
    }
   ],
   "source": [
    "validation_df = pd.read_csv('/home/ec2-user/SageMaker/imat/train.csv')\n",
    "\n",
    "eval_path_list = validation_df['imagePath']\n",
    "\n",
    "eval_data = load_images(eval_path_list)\n",
    "eval_labels = get_multi_hot_labels(validation_df, list(range(validation_df.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/ec2-user/SageMaker/imat/model/multilabel_alexnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe59d16c048>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-02-11T21:28:05Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/SageMaker/imat/model/multilabel_alexnet_model/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-02-11-21:28:24\n",
      "INFO:tensorflow:Saving dict for global step 5000: global_step = 5000, loss = 0.109409556, meanfscore = 0.37077868, precision_micro = 0.31033868, recall_micro = 0.4604544\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /home/ec2-user/SageMaker/imat/model/multilabel_alexnet_model/model.ckpt-5000\n",
      "{'loss': 0.109409556, 'meanfscore': 0.37077868, 'precision_micro': 0.31033868, 'recall_micro': 0.4604544, 'global_step': 5000}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "multilabel_classifier = tf.estimator.Estimator(\n",
    "            model_fn=alexnet_model_fn, model_dir=\"/home/ec2-user/SageMaker/imat/model/multilabel_alexnet_model\")\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": eval_data},\n",
    "            y=eval_labels,\n",
    "            shuffle=False)\n",
    "eval_results = multilabel_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
